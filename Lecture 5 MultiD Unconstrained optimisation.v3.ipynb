{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lecture 5\n",
      "===========================================================================\n",
      "\n",
      "Multivariable unconstrained optimisation\n",
      "------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sympy import *\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from matplotlib import cm\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "%matplotlib inline\n",
      "init_printing(use_latex='mathjax')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Techniques which require only function values\n",
      "==========\n",
      "\n",
      "Downhill Simplex or Sequential Simplex method\n",
      "------------\n",
      "\n",
      "A simplex is simply a polytope (multidimensional version of polygon) with $n+1$ vertices (points) in $n$-dimensional space\n",
      "\n",
      "In two dimensions, this is a triangle, in three, it is a tetrahedron\n",
      "\n",
      "The Nelder-Mead version of the simplex algorithm is very commonly implemented in numerical libraries and has the advantage of only needing function values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x1, x2 = symbols('x1 x2')\n",
      "f = (x1 - 3)**2 + (x2 - 3)**2 + (x1 - 3)*(x2 - 3)\n",
      "f"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def gradient(f, variables):\n",
      "    return [f.diff(variable) for variable in variables]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = gradient(f, [x1, x2])\n",
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "minxy = solve([df[0], df[1]], [x1, x2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "minxy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def plotfunc(f, xlim, ylim, plotquiver=None, reddot=None):\n",
      "    evalf = lambdify((x1, x2), f, modules=['numpy'])\n",
      "    xl = np.linspace(xlim[0], xlim[1], 20)\n",
      "    yl = np.linspace(xlim[0], xlim[1], 20)\n",
      "    xx, yy = np.meshgrid(xl, yl)\n",
      "    zz = evalf(xx, yy)\n",
      "    plt.contourf(xx, yy, zz, cmap='binary_r')\n",
      "    if plotquiver:\n",
      "        df = gradient(f, [x1, x2])\n",
      "        evalgrad = [lambdify((x1, x2), dfi, modules=['numpy']) for dfi in df]\n",
      "        vx1, vx2 = [-evd(xx, yy) for evd in evalgrad]\n",
      "        plt.quiver(xx, yy, vx1, vx2, headwidth=4, headlength=6, color='r')\n",
      "    if reddot:\n",
      "        plt.scatter(reddot[x1], reddot[x2], s=100, color='red')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plotfunc(f, (0., 5.), (0., 5.), plotquiver=True, reddot=minxy)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import numpy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# simple simplex method implementation - only reflection\n",
      "def simplex_method(f, alpha, N, simplex0):\n",
      "    evalf = lambdify((x1, x2), f)\n",
      "    simplex = simplex0\n",
      "    simplexes = [simplex] # initial simplex\n",
      "    for i in range(N):\n",
      "        # Evaluate all points in simplex\n",
      "        fs = [evalf(*point) for point in simplex]\n",
      "        # sort points in simplex according to function value (so the last point is the worst)\n",
      "        sortedfs, sortedsimplex = zip(*sorted(zip(fs, simplex), key=lambda x: x[0]))\n",
      "        # calculate center of all but last point\n",
      "        center = numpy.mean(sortedsimplex[:-1], axis=0)\n",
      "        reflectedpoint = center + alpha*(center - sortedsimplex[-1])\n",
      "        simplex = list(sortedsimplex[:-1]) + [reflectedpoint]\n",
      "        simplexes.append(simplex)\n",
      "    return simplexes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "initialsimplex = [numpy.array(v) for v in [(0, 0), (0.2, 0.9), (0.9, 0.2)]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "simplex_method(f, 1, 4, initialsimplex)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.patches as patches\n",
      "from IPython.html.widgets import interact, interactive\n",
      "from IPython.html import widgets\n",
      "from IPython.display import display"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def show_simplex(alpha, N):\n",
      "    simplex = simplex_method(f, alpha, N, initialsimplex)\n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(111)\n",
      "    plotfunc(f, (0, 5), (0, 5), reddot=minxy)\n",
      "    for i in range(np.shape(simplex)[0]):\n",
      "        poly = patches.Polygon(simplex[i], edgecolor='r', facecolor='none')\n",
      "        ax.add_patch(poly)\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sm = interactive(show_simplex, N=widgets.IntSlider(min=1.0,max=50.0,step=1.0,value=1.0), alpha=(0.0,1.0,0.01))\n",
      "display(sm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Conjugate direction search\n",
      "------------\n",
      "Vectors $\\mathbf{s}^i$ and $\\mathbf{s}^j$ are said to conjugate with respect to matrix Q if \n",
      "\n",
      "${\\mathbf{s}^i}^TQ\\mathbf{s}^j=0$\n",
      "\n",
      "The set of directions which satisfy these equations is known as the nullspace or kernel of the matrix. For optimisation the matrix Q is the Hessian matrix.\n",
      "\n",
      "Example 6.1:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x1, x2 = symbols('x1 x2')\n",
      "f = 2*x1**2 + x2**2 - 3\n",
      "H = hessian(f, [x1,x2])\n",
      "f, H"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "minxy = solve(gradient(f, [x1, x2]), [x1, x2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start the search at the point [1,1] with an initial direction ${\\mathbf{s}^i}^T$ = [-4,-2]. To find the conjugate direction we determine the nullspace of ${\\mathbf{s}^i}^TQ$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s1 = Matrix([-4, -2]).T\n",
      "A = s1*H\n",
      "s1conj = A.nullspace()[0]\n",
      "s1conj"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To be consistent with the textbook we multiply this vector by -4, an operation which does not change it. To be sure we can check the original requirement:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s1conj = s1conj*(-4)\n",
      "s1*H*s1conj"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The original direction and the conjugate directions are indicated on the figure."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def plot_vector(p1, p2, color='b'):\n",
      "    ax = plt.gca()\n",
      "    ax.arrow(float(p1[0]), float(p1[1]), float(p2[0]), float(p2[1]), \n",
      "             head_width=0.5, head_length=0.5, \n",
      "             fc=color, ec=color)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plotfunc(f, (-5, 5), (-5, 5), reddot=minxy)\n",
      "plot_vector((1, 1), (-4, -2), 'r')\n",
      "plot_vector((1, 1), s1conj, 'g')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To find the optimal value we must take a step in the initial direction:\n",
      "   ${\\mathbf{x}_{new} = \\mathbf{x}_{old} + \\alpha\\mathbf{s}}$\n",
      "\n",
      "Or for the individual directions:\n",
      "\n",
      "$${{x}_{1,new} = {x}_{1,old} + \\alpha{s}_{1}}$$\n",
      "\n",
      "$${{x}_{2,new} = {x}_{2,old} + \\alpha{s}_{2}}$$\n",
      "\n",
      "To determine the optimal step size we conduct a line search in the initial direction, for convenience we keep the step size fixed as opposed to before when we gradually increased it. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def vec_wrap(func):\n",
      "    lambdified_func = lambdify((x1, x2), func)\n",
      "    def vector_evaluator(x):\n",
      "        return numpy.array(lambdified_func(*x))\n",
      "    return vector_evaluator"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alpha = 0.01\n",
      "s0 = Matrix([1.0, 1.0])\n",
      "spoints = [s0]\n",
      "newvals = s0 + alpha * s1.T\n",
      "spoints.append(newvals)\n",
      "k = 1\n",
      "\n",
      "while f.subs([(x1,spoints[k][0]),(x2,spoints[k][1])]) <  f.subs([(x1,spoints[k-1][0]),(x2,spoints[k-1][1])]):\n",
      "    k = k + 1\n",
      "    newvals = s0 + alpha * k * s1.T\n",
      "    spoints.append(newvals)\n",
      "\n",
      "print alpha * k    "
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "The search indicates that we will be at the minimum value of f along the direction ${\\mathbf{s}^i}^T$ = [-4,-2] if we multiply this vector by 0.29, this makes the new starting point:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s2 = s0 + alpha * k * s1.T\n",
      "s2"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots()\n",
      "plotfunc(f, (-5, 5), (-5, 5), reddot=minxy)\n",
      "plot_vector((1, 1), (-4, -2), 'r')\n",
      "plot_vector(s2, s1conj, 'g')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "Performing another line search in the conjugate direction from this new starting point:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alpha = 0.01\n",
      "spoints = [s2]\n",
      "newvals = s2 + alpha * s1conj\n",
      "spoints.append(newvals)\n",
      "k = 1\n",
      "evalf = vec_wrap(f)\n",
      "while evalf(spoints[k]) <  evalf(spoints[k-1]):\n",
      "    k = k + 1\n",
      "    newvals = s2 + alpha * k * s1conj\n",
      "    spoints.append(newvals)\n",
      "\n",
      "print alpha * k   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Thus if we multiply the conjugate vector by 0.12 and add it to the starting point we obtain the optimal value:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s2 + alpha * k * s1conj"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is very close to the true optimum of (0,0), to obtain a better value smaller steps of ${\\alpha}$ could be used. The reason we achieve the optimal solution in only two steps is because the function is quadratic."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Techniques which require function derivatives\n",
      "==========\n",
      "\n",
      "Conjugate gradient search\n",
      "------------\n",
      "This approach combines the use of conjugate directions with the use of the gradient \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Newton's method\n",
      "------------\n",
      "This approach combines the use of conjugate directions with the use of the gradient \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Built-in functions\n",
      "---------------\n",
      "\n",
      "SciPy offers implementations of some of the methods discussed in the book in the standard entry point `scipy.optimize.minimise`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import scipy.optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's unleash a couple of methods on the Rosenbrock function (the function from Example 6.2)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import numpy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x1, x2 = symbols('x1 x2')\n",
      "rosenbrock = 100*(x2 - x1**2)**2 + (1 - x1)**2\n",
      "rosenjac = [diff(rosenbrock, v) for v in [x1, x2]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = vec_wrap(rosenbrock)\n",
      "df = vec_wrap(rosenjac)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for algorithm in ['Nelder-Mead', 'Powell', 'BFGS', 'Newton-CG']:\n",
      "    print algorithm\n",
      "    print scipy.optimize.minimize(f, [-1.2, 1], method=algorithm, jac=df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}
